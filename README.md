# Enterprise GenAI Assistant (Local RAG-style)

An **Enterprise GenAI Assistant** built using **local LLM inference (llama.cpp + GGUF models)** that can ingest enterprise documents, split them into context chunks, and generate **context-aware answers** without relying on external APIs such as OpenAI.

This project focuses on **reliability, privacy, and explainability**, and demonstrates practical GenAI system design rather than a black-box API wrapper.

---

## ğŸš€ Key Features

- ğŸ“„ **Enterprise document ingestion** (text-based documents)
- âœ‚ï¸ **Configurable text chunking** for long documents
- ğŸ§  **Context-augmented generation (RAG-style baseline)**
- ğŸ”’ **Fully local inference** using GGUF models (no data leaves the machine)
- âš™ï¸ **No external API dependency**
- ğŸ§© Clean, modular architecture suitable for extension (vector DB, hybrid search, etc.)

---

## ğŸ—ï¸ Architecture Overview

```
Documents (.txt)
      â†“
Text Loader
      â†“
Text Splitter (chunking)
      â†“
Context Selection (top-N chunks)
      â†“
Prompt Construction
      â†“
Local LLaMA Inference (llama.cpp)
      â†“
Generated Answer
```

> **Note:** This project implements a **robust RAG baseline**. Vector database integration (Qdrant / LanceDB / FAISS) can be added later as an enhancement.

---

## ğŸ“‚ Project Structure

```
enterprise-genai-assistant/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ enterprise-assistant.js   # Main entry point
â”‚   â”œâ”€â”€ llms/                      # LLM wrappers (LlamaCpp)
â”‚   â”œâ”€â”€ text-splitters/            # Chunking logic
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ enterprise_docs/           # Input documents (.txt files)
â”‚
â”œâ”€â”€ models/                        # GGUF model files
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

- **Node.js** >= 18
- **npm** or **pnpm**
- A **GGUF LLM model** compatible with `llama.cpp`
- Minimum **8GB RAM** recommended for 1â€“3B models

---

## ğŸ¤– Supported Models (Examples)

You can use any GGUF model supported by `node-llama-cpp`.

Examples:
- `Qwen3-1.7B-Q8_0.gguf`
- `Llama-3.2-1B-Instruct.gguf`

> Smaller models are recommended for laptops.

---

## ğŸ“¥ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Harshith743/enterprise-genai-assistant.git
cd enterprise-genai-assistant
```

---

### 2ï¸âƒ£ Install dependencies

```bash
npm install
```

---

### 3ï¸âƒ£ Download a GGUF model

Example using `node-llama-cpp`:

```bash
npx node-llama-cpp pull hf:Qwen/Qwen3-1.7B-GGUF:Q8_0 --dir ./models
```

After download, ensure you have a file like:

```
models/Qwen3-1.7B-Q8_0.gguf
```

---

### 4ï¸âƒ£ Add enterprise documents

Place your documents inside:

```
examples/enterprise_docs/
```

Supported format:
- `.txt`

Example:
```
examples/enterprise_docs/
â”œâ”€â”€ company_policy.txt
â”œâ”€â”€ internal_guidelines.txt
```

---

### 5ï¸âƒ£ Configure model path

Open `src/enterprise-assistant.js` and update:

```js
modelPath: "./models/Qwen3-1.7B-Q8_0.gguf",
```

Make sure this matches the **exact filename** in your `models/` directory.

---

## â–¶ï¸ How to Run

```bash
node src/enterprise-assistant.js
```

### Example Output

```
ğŸ“„ Loading enterprise documents...
âœ… Loaded 2 documents
âœ‚ï¸ Splitting documents into chunks...
âœ… Created 344 chunks

â“ User Query:
Summarize the information available in the enterprise documents.

ğŸ’¡ Answer:
<Generated summary>
```

---

## ğŸ§  How It Works (Detailed)

1. **Loads enterprise documents** from a directory
2. **Splits text into overlapping chunks** to fit LLM context limits
3. **Selects top-N chunks** as context (baseline retrieval)
4. **Constructs a grounded prompt** using selected context
5. **Runs local LLM inference** using llama.cpp

This approach provides a **transparent and explainable RAG-style pipeline**.

---

## ğŸ” Privacy & Security

- No external API calls
- No document data leaves the system
- Suitable for enterprise / internal knowledge use cases

---

## ğŸš§ Limitations

- No vector database (yet)
- Retrieval is heuristic-based (top-N chunks)
- Supports text documents only

These are **intentional design choices** for clarity and stability.

---

## ğŸ”® Future Improvements

- Vector database integration (Qdrant / LanceDB)
- Semantic retrieval + re-ranking
- PDF and DOCX loaders
- Streaming responses
- Web UI

---

## ğŸ“Œ Resume Description

> **Enterprise GenAI Assistant**  
> Built a document-grounded GenAI assistant using local LLaMA (GGUF) inference. Implemented enterprise document ingestion, configurable text chunking, and context-aware response generation without relying on external APIs. Focused on privacy, reliability, and explainable system design.

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

**Harshith Reddy T**  
GitHub: https://github.com/Harshith743

---

â­ If you found this project useful, feel free to star the repository.

